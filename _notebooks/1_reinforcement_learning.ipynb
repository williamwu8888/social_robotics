{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb43a82f",
   "metadata": {},
   "source": [
    "# MU5EEH15: Interactive Robot Learning\n",
    "\n",
    "Objective: Learn how to do programming in `Python` interactive robot learning.\n",
    "- Machine learning / Human Robot Interaction (HRI)\n",
    "- Reinforcement learning (rewards, human feedback)\n",
    "- Supervised learning\n",
    "- Immitation learning\n",
    "\n",
    "**Organization**: Lectures and Practical Labs **(TP 40%)** + Final project **(40%)** and exam **(20%)**\n",
    "\n",
    "**Lecturer**: Silvia TULLI - mail: silvia.tulli@sorbonne-universite.fr\n",
    "\n",
    "**Student**: William WU - mail: william.wu@etu.sorbonne-universite.fr\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c62a78",
   "metadata": {},
   "source": [
    "# Agenda 29/09/2025\n",
    "\n",
    "Recap Questionnaire on Human-Interactive Robot Learning\n",
    "- Fundamentals of RL Course\n",
    "- Practice with Jupyter Notebooks\n",
    "- Final Questionnaire on today's material, both course and practice\n",
    "\n",
    "# Learning Goals\n",
    "\n",
    "By the end of this lecture, you should be able to:\n",
    "- Frame a sequential decision making problem as an MDP\n",
    "- Explain the concept of value function (V) and action function (Q)\n",
    "- Apply value iteration and policy iteration to solve an MDP\n",
    "- Explain general RL concepts such as Model-based vs. Model-free RL and the exploration-exploitation tradeoff\n",
    "- Master the following algorithms :Value Iteration, Policy Iteration, Dynamic Programming, Q-learning, SARSA.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb9c42",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Sequential Decision Making - Simple Gridworld](#sequential-decision-making---simple-gridworld)\n",
    "    - [Markov Decision Process](#markov-decision-process)\n",
    "    - [Different Policies](#different-policies)\n",
    "2. [Machine Learning Paradigms](#machine-learning-paradigms)\n",
    "    - [Reinforcement Learning Framework](#reinforcement-learning-framework)\n",
    "    - [Sequential Decision Problems (MDPs/POMDPs)](#sequential-decision-problems-mdpspomdps)\n",
    "    - [Basic Decision-Making Problem (Deterministic)](#basic-decision-making-problem-deterministic) \n",
    "    - [Principle of Optimality (Bellman's Principle)](#principle-of-optimality-bellmans-principle)\n",
    "    - [Rewards](#rewards)\n",
    "    - [Exploration/Exploitation Tradeoff](#explorationexploitation-tradeoff)\n",
    "    - [Terminology](#terminology)\n",
    "    - [Offline vs Online Planning](#offline-vs-online-planning)\n",
    "    - [Solving MDPs](#solving-mdps)\n",
    "    - [Q-learning vs Deep Q-learning](#q-learning-vs-deep-q-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd5e41",
   "metadata": {},
   "source": [
    "# Sequential Decision Making - Simple Gridworld\n",
    "\n",
    "- Robot/Mouse (agent)\n",
    "- Grab the cheese (goal)\n",
    "- Maze (environment)\n",
    "- Cells of the maze (possible states)\n",
    "- Arrows (agent’s possible actions)\n",
    "\n",
    "Noisy environment - if you move forward you do not necessarily go forward\n",
    "\n",
    "After the learning happens and after we realise how to solve it optimally that would be a potential trajectory when everything is done\n",
    "\n",
    "## Markov Decision Process\n",
    "\n",
    "The \"Markov\" in \"Markov decision process\" refers to the underlying structure of state transitions that still follow the Markov property. MDP is a model for sequential decision making. The Markov property means that evolution of the Markov process in the future depends only on the present state and does not depend on past history. \n",
    "\n",
    "## Different Policies\n",
    "\n",
    "Policies are the way for programs to aim their objectives.\n",
    "- Supervised policy is an algorithm updated with human wants that orientates the policy\n",
    "- Greedy policy is maximizing the reward.\n",
    "- Epsilon-greedy policy is a policy that introduces epsilon which sets the level of exploring the program will do\n",
    "\n",
    "___\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b5186",
   "metadata": {},
   "source": [
    "# Machine Learning Paradigms\n",
    "\n",
    "- Supervised learning: labeled data, with training on labels and then do predictions.\n",
    "- Unsupervised learning: no label data, feature extractions (no ground truths) then classification.\n",
    "- Reinforcement learning: chasing for rewards (different algorithms)\n",
    "\n",
    "## Reinforcement Learning Framework\n",
    "\n",
    "<img src=\"RL_diagram.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "When we do reinforcement learning:\n",
    "- There is no supervisor, only a reward signal (e.g., this was good/bad, this gives you 10 points)\n",
    "- Feedback may be delayed, but can also be instantaneous depending on the environment\n",
    "- Time really matters: we talk about sequential processes (non i.i.d data)\n",
    "- The agent is influenced by the sequence of data it receives\n",
    "\n",
    "Reinforcement learning is the science of decision-making → try to find the optimal way to make decisions A number of impressive successes in the last decade.\n",
    "\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ad7a9",
   "metadata": {},
   "source": [
    "## Sequential Decision Problems (MDPs/POMDPs)\n",
    "\n",
    "- **Goal**: select actions to maximise total future reward\n",
    "- Actions may have **long term consequences**\n",
    "- **Reward** may be delayed \n",
    "- It may be better to **sacrifice** immediate reward to gain more long-term reward\n",
    "- Actions change the environment state\n",
    "- Actions may have rewards that appear many steps later\n",
    "- The **sequence of actions matters**, not just individual choices\n",
    "- Hard to know which past actions led to current rewards\n",
    "\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d3057",
   "metadata": {},
   "source": [
    "## Basic decision-making problem (deterministic)\n",
    "\n",
    "- **Aim:** Find the best sequence of control actions to minimize some cost while satisfying system dynamics and constraints\n",
    "- Discrete-time model with additive cost (central assumption)\n",
    "\n",
    "#### System: $ x_{k+1} = f_k(x_k, u_k), \\quad k = 0, \\ldots, N $\n",
    "\n",
    "- $x_{k+1}$ is the condition: state vector at time step k\n",
    "- $u_k$ is the action: control input at time step k\n",
    "- $f_k(x_k, u_k)$ is the state transition function (how the system evolves from one time step to the next)\n",
    "- The next state depends on the current state and the control we apply\n",
    "- No randomness; given state and control, next state is known\n",
    "\n",
    "#### Control constraints: $ u_k \\in U(x_k) $\n",
    "\n",
    "- This represents physical limitations (e.g., maximum motor torque, speed limits)\n",
    "- The constraints can be state-dependent, meaning available actions may change based on where you are\n",
    "\n",
    "#### Cost: $ J(x_0; u_0, \\ldots, u_{N-1}) = g_N(x_N) + \\sum_{k=0}^{N-1} g_k(x_k, u_k) $\n",
    "\n",
    "- $g_N(x_N)$ is the terminal cost (penalty for where we end up)\n",
    "- $g_k(x_k, u_k)$ is the stage cost at each time step (running costs)\n",
    "- The cost combines: immediate costs at each step + final cost\n",
    "\n",
    "#### Decision-Making Problem: $ J^*(x_0) = \\min_{u_k \\in U(x_k), k=0, \\ldots, N-1} J(x_0; u_0, \\ldots, u_{N-1}) $\n",
    "\n",
    "- Optimal value function (minimum achievable cost from initial state $x_0$)\n",
    "- We're finding the control sequence that minimizes total cost. \n",
    "- Subject to: system dynamics + control constraints\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21cf6d",
   "metadata": {},
   "source": [
    "## Principle of optimality (Bellman's Principle)\n",
    "\n",
    "Let $\\{u_0^*,u_1^*,...u_{N-1}^*\\}$ be an optimal control sequence, which together with $x_0^*$ determines the corresponding state sequence $\\{x_0^*,x_1^*,...x_N^*\\}$. Consider the subproblem whereby we are at $x_k^*$ at time $k$. \n",
    "\n",
    "We wish to minimize the cost-to-go from time $k$ to time $N$, i. e.,\n",
    "\n",
    "$ g_k(x_k^*,u_k) + \\sum_{m=k+1}^{N-1} g_m(x_m,u_m) + g_N(x_N) $\n",
    "\n",
    "Then the truncated optimal sequence $\\{u_0^*,u_1^*,...u_{N-1}^*\\}$ is optimal for the subproblem. \n",
    "\n",
    "Tail of optimal sequences optimal for tail subproblems\n",
    "\n",
    "#### Applying the principle of optimality\n",
    "\n",
    "- need only to compare the concatenations of immediate decisions and optimal decisions \n",
    "    - significant decrease in computation / possibilities\n",
    "- in practice: carry out this procedure backward in time\n",
    "\n",
    "#### Dynamic Programming Algorithm\n",
    "\n",
    "Start with $ J_N^*(x_N) = g_N(x_N) $, for all $ x_N $\n",
    "\n",
    "and for $ k = N - 1, \\ldots, 0 $, let\n",
    "\n",
    "$\n",
    "J_k^*(x_k) = \\min_{u_k \\in U(x_k)} \\left[ g(x_k, u_k) + J_{k+1}^* (f(x_k, u_k)) \\right] \\quad ,\\forall x_k\n",
    "$\n",
    "\n",
    "Once the functions $ J_0^*, \\ldots, J_N^* $ have been determined, the optimal sequence can be determined with a forward pass.\n",
    "\n",
    "Bellman's principle: the optimal cost from state $x_k$ equals the minimum over all feasible controls of [immediate cost + optimal future cost].\n",
    "\n",
    "After computing backwards all value functions you construct the actual optimal trajectory\n",
    "\n",
    "- DP guarantees finding the globally optimal solution (not just locally optimal) due to the principle of optimality\n",
    "- The algorithm must be computed for all possible states $x_k$, which can be computationally challenging in high-dimensional problems.\n",
    "- Instead of computing the values over the entire state space via DP, we can be smarter by only computing the values around our best-guess trajectory and iteratively closing in on the actual optimal solution.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c01856",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "- First step is understanding the reward signal\n",
    "- Scalar feedback $R_t$ about how well the agent is doing at time step $t$\n",
    "- The goal of the agent is to maximise the cumulative reward\n",
    "- Reinforcement learning is based on the reward hypothesis.\n",
    "\n",
    "All goals can be expressed by the maximisation of expected cumulative rewards\n",
    "\n",
    "## Exploration/exploitation tradeoff\n",
    "\n",
    "To figure out what is the meaning of the scalar value the agent’s has to explore Exploration/exploitation tradeoff\n",
    "- Should the agent explores what it knows or looks for new solutions?\n",
    "- How fast should you decrease your exploration rate?\n",
    "- Epsilon-greedy: taking the best action most of the time and a random action from time to time\n",
    "\n",
    "## Terminology\n",
    "\n",
    "- A learning agent has to:\n",
    "    - Sense the state of its environment\n",
    "    - Take actions that affect the state\n",
    "    - Have a goal or goals relating to the state of the environment\n",
    "\n",
    "- A Markov Decision Process (MDP) is a mathematical formalisation for modeling decision making and include these aspects:\n",
    "    - Sensation, Action, Goal\n",
    "\n",
    "- Policy - maps from perceived states of the environment to actions to be taken in those states (e.g., in psychology stimulus-response rules or associations)\n",
    "- Reward signal - defines the goal in the problem. Single number that tells what are the good and bad events for the agent (e.g., in biological systems pleasure/pain)\n",
    "- Value Function - specifies what is good in the long run\n",
    "- Model of the environment - mimics the behavior of the environment\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd24c81",
   "metadata": {},
   "source": [
    "## Offline vs Online Planning\n",
    "\n",
    "- Offline planning (MDPs)\n",
    "    - MDP is given\n",
    "    - The agent find the optimal policy for the MDP\n",
    "    - The agent acts in the environment\n",
    "\n",
    "- Online Planning (RL)\n",
    "    - Learning is required\n",
    "    - The agent has access to a set of available actions and information about the state it is in.\n",
    "\n",
    "\n",
    "## Solving MDPs\n",
    "\n",
    "Techniques for solving MDPs (and POMDPs) can be separated into three categories:\n",
    "- **Value-based techniques** aim to _learn the value of states_ (or learn an estimate for value of states) and actions: that is, they learn value functions or Q functions. We then use _policy extraction_ to get a policy for deciding actions.\n",
    "- **Policy-based techniques** _learn a policy directly_, which _completely by-passes learning values of states or actions all together_. It's important if, for example, the state space or the action space are massive or infinite. If the action space is infinite, then using policy extraction is not possible because we must iterate over all actions to find the optimal one. If we\n",
    "learn the policy directly, we do not need this.\n",
    "- **Hybrid techniques** that combine _value and policy-based techniques_.\n",
    "\n",
    "## Q-learning vs Deep Q-learning\n",
    "\n",
    "<img src=\"value_iteration.png\" width=\"800\" height=\"500\">\n",
    "<img src=\"policy_iteration.png\" width=\"800\" height=\"500\">\n",
    "\n",
    "\n",
    "*she went speed-running for the slides, gotta read now...*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b46f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
