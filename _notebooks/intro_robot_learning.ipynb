{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1484acd-e317-4ebb-b864-77226ae19b8b",
   "metadata": {},
   "source": [
    "# MU5EEH15: Interactive Robot Learning\n",
    "\n",
    "**Objective**: Learn how to do programming in `Python` interactive robot learning.\n",
    "- Machine learning / Human Robot Interaction (HRI)\n",
    "- Reinforcement learning (rewards, human feedback)\n",
    "- Supervised learning\n",
    "- Immitation learning\n",
    "\n",
    "**Organization**: Lectures and Practical Labs **(TP 40%)** + Final exams **(60%)**\n",
    "\n",
    "**Evaluation**: C++ Programming exam **(30%)** + ROS Driver Design exam **(30%)**\n",
    "\n",
    "**Teacher**: Mohamed CHETOUANI - mail: mohamed.chetouani@sorbonne-universite.fr\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6398a1ce-50cd-416f-a776-33d6610bef80",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Organization](#organization)\n",
    "- [Introduction](#introduction)\n",
    "- [Machine Learning (ML) and Interactive ML](#machine-learning-ml-and-interactive-ml)\n",
    "- [Learning to summarize from human feedback](#learning-to-summarize-from-human-feedback)\n",
    "- [Agents Learning from Human Teachers](#agents-learning-from-human-teachers)\n",
    "- [Robot Social Learning](#robot-social-learning)\n",
    "  - [Social Interaction Between Learner and Tutor](#social-interaction-between-learner-and-tutor)\n",
    "  - [Beliefs, Desires, and Intentions (BDI) in Interactive Learning](#beliefs-desires-and-intentions-bdi-in-interactive-learning)\n",
    "  - [Ostensive Signals (Attention)](#ostensive-signals-attention)\n",
    "  - [Communication in Action](#communication-in-action)\n",
    "- [Humans in the Machine Learning Process](#humans-in-the-machine-learning-process)\n",
    "  - [a) Standard Imitation Learning](#a-standard-imitation-learning)\n",
    "  - [b) Evaluative Feedback](#b-evaluative-feedback)\n",
    "  - [c) Imitation from Observation](#c-imitation-from-observation)\n",
    "  - [d) Learning Attention from Humans](#d-learning-attention-from-humans)\n",
    "  - [e) Learning from Human Preference](#e-learning-from-human-preference)\n",
    "  - [f) Hierarchical Imitation](#f-hierarchical-imitation)\n",
    "- [Teaching and Learning Costs](#teaching-and-learning-costs)\n",
    "  - [A) Imitation](#a-imitation)\n",
    "  - [B) Feedback](#b-feedback)\n",
    "- [Interactive Robot Learning Paradigm](#interactive-robot-learning-paradigm)\n",
    "- [Human Strategies: Example of graspable objects](#human-strategies)\n",
    "- [Conclusion](#conclusion)\n",
    "\n",
    "# Organization\n",
    "\n",
    "- Introduction to Interactive: Machine Learning\n",
    "- Strategies to teach machines: How do humans teach machines ?\n",
    "- Learning from evaluative feedback and/or demonstrations\n",
    "- Open challenges in Interactive Robot Learning\n",
    "___\n",
    "\n",
    "# Introduction\n",
    "\n",
    "*Don't hesitate to take a look to the document [Open PDF](InteractiveRobotLearning.pdf)...*\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78980c3d",
   "metadata": {},
   "source": [
    "# Machine Learning (ML) and Interactive ML\n",
    "\n",
    "For Machine Learning, we usually need the following:\n",
    "- Define the **features** (what choices does the machine make),\n",
    "- Define the **metrics** (accuracy, precision, recall, F1-score…),\n",
    "- Get to **know**/**understand** the problem (ask the users: data, insights, labels…),\n",
    "- Choose the **design/algorithm** (random forest, support vector machine, k-means, etc.),\n",
    "- In neural networks:\n",
    "  - Define the **architecture** (layers, neurons, activation functions),\n",
    "  - Specify the **loss function** and optimization method,\n",
    "  - Use **gradient-based optimization** (e.g., gradient descent, backpropagation) to adjust weights,\n",
    "  - Train with **labeled data** until convergence.\n",
    "\n",
    "With Interactive Machine Learning,\n",
    "- We can, as humans, directly **teach the robot** through **feedback** (reinforcement, corrections, demonstrations),\n",
    "- The learning is **incremental and adaptive**, guided by user input instead of only large offline datasets,\n",
    "- Adjustments can also rely on **gradient-based updates**, but influenced by **human feedback** rather than just static loss functions.\n",
    "- Needs **less data** because human feedback is **targeted and informative**, correcting mistakes directly and guiding the model toward the right solution without requiring thousands of redundant examples.\n",
    "\n",
    "___\n",
    "\n",
    "# Learning to Summarize from Human Feedback\n",
    "\n",
    "#### 1. Sample Selection\n",
    "- Humans typically provide a **large set of samples** (examples, demonstrations, corrections).\n",
    "- The robot learns to identify and generate a **smaller set of preferred samples**, focusing on the most relevant or useful ones.\n",
    "- This process is known as **Preference Learning**.\n",
    "\n",
    "#### 2. Training with Human Preferences\n",
    "- Training involves concepts like **loss functions**, **entropy**, and **reward signals**.\n",
    "- The robot may be given two candidate summaries, and a human indicates **which one is better**.\n",
    "- This resembles supervised learning, but only for the **preference selection step**, not the entire learning process.\n",
    "\n",
    "#### 3. Design\n",
    "- The model architecture is typically a **neural network** capable of scoring or ranking candidate summaries.\n",
    "- The **loss function** is designed to **maximize the preference agreement** with human feedback.\n",
    "- Training may include **gradient-based updates** guided by the reward signal from human preferences.\n",
    "- The system is designed to **generalize from few examples**, leveraging the targeted human feedback efficiently.\n",
    "\n",
    "___\n",
    "\n",
    "# Agents Learning from Human Teachers\n",
    "\n",
    "**Objective:** Learning from a human teacher → Interactive task learning\n",
    "(It refers to any process by which an agent learns:\n",
    "(i) to communicate about a task, and\n",
    "(ii) to perform a task through natural interaction with a human.)\n",
    "\n",
    "**Focus on teaching signals:**\n",
    "- Human teaching signals are **multimodal cues** communicated through different **forms**, such as feedback, demonstrations, or instructions.\n",
    "- These signals can be more or less informative, being **explicit** (clear choice) or **implicit**, and aim to **intentionally** shape the agent's behavior.\n",
    "- How can an agent make sense of all these diverse teaching signals?\n",
    "- Flow: **Human teacher → teaching signals → learning agent → feedback to human teacher**\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579f338",
   "metadata": {},
   "source": [
    "# Robot Social Learning\n",
    "\n",
    "## Social Interaction Between Learner and Tutor\n",
    "- Learning occurs through **interaction between the learner (robot) and the tutor (human)**.\n",
    "- Signals can be divided into two main channels:\n",
    "  1. **Task-channel signals** – communication about the **task itself**, such as goals, actions, or outcomes.\n",
    "  2. **Social-channel signals** – communication about the **social context**, such as encouragement, attention, or guidance.\n",
    "- Task-channel signals help in **communicating the robot's understanding and performance of the task**.\n",
    "- Social-channel signals help the robot interpret **intentions and expectations of the human tutor**.\n",
    "\n",
    "## Beliefs, Desires, and Intentions (BDI) in Interactive Learning\n",
    "- **Beliefs (hypotheses):** The robot’s understanding of the world, including task states and human cues.\n",
    "- **Desires (goals):** The objectives or goals the robot wants to achieve (aligned with the human tutor’s goals).\n",
    "- **Intentions (plans):** The specific actions the robot commits to in order to achieve its goals (*complex for humans*).\n",
    "- The robot can **reason about human instructions, anticipate outcomes, and plan actions** more effectively within both task and social channels.\n",
    "\n",
    "## Ostensive Signals (Attention)\n",
    "\n",
    "- Ostensive signals are cues that indicate **communication is intended for the learner**.\n",
    "- In humans, studies show that the **infant brain responds strongly to dynamic mutual and averted gaze stimuli**, which signal attention and intention from the caregiver.\n",
    "- These signals help the learner **detect relevant information and understand the teacher’s intention**, forming the basis for social learning.\n",
    "\n",
    "## Communication in Action\n",
    "\n",
    "- An **actor** (human or robot) intends not just to **perform an action**, but also to **convey information about the action**.\n",
    "- This involves two complementary processes:\n",
    "  1. **Planning and Acting (A → C):** Deciding and executing the action while embedding communicative intent.\n",
    "  2. **Inference (B → C):** Observers infer the actor's intention or meaning from the action.\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb59fc-1cb2-4947-a24c-c0934962667b",
   "metadata": {},
   "source": [
    "\n",
    "# Humans in the Machine Learning Process\n",
    "\n",
    "*Slide 16 [Open PDF: slides](InteractiveRobotLearning_slides.pdf)*\n",
    "\n",
    "- Humans play a central role in **guiding and supervising the learning process**.\n",
    "- Key contributions include:\n",
    "  - **Data collection and labeling** – providing high-quality datasets for training.\n",
    "  - **Feature selection and engineering** – helping define what information the machine should focus on.\n",
    "  - **Designing reward or feedback signals** – enabling interactive or reinforcement learning.\n",
    "  - **Interpreting results and correcting errors** – guiding the model toward better performance.\n",
    "- In **Interactive Machine Learning**, humans act as **teachers**, giving targeted feedback, demonstrations, or preferences to shape learning efficiently.\n",
    "\n",
    "#### a) Standard Imitation Learning\n",
    "- The robot **learns by observing human demonstrations** of a task.\n",
    "- It tries to **replicate the demonstrated behavior** without explicit programming.\n",
    "- Useful when **explicit reward functions are difficult to define**.\n",
    "- Focuses on **copying correct actions** rather than evaluating performance.\n",
    "\n",
    "#### b) Evaluative Feedback\n",
    "- The robot improves based on **human-provided evaluations** of its actions.\n",
    "- Feedback can indicate **good/bad performance** or **how well the task was done**.\n",
    "- Enables **incremental and interactive learning**, guiding the robot toward better behavior.\n",
    "- Often combined with **preference learning** or **gradient-based updates** for efficiency.\n",
    "\n",
    "#### c) Imitation from Observation\n",
    "- The robot **learns by passively observing human actions** without explicit instructions.\n",
    "- Focuses on **extracting patterns and goals** from observed behavior.\n",
    "- Useful when **direct demonstrations or guidance are limited**.\n",
    "- Enables the robot to **generalize actions** in similar contexts.\n",
    "\n",
    "#### d) Learning Attention from Humans\n",
    "- The robot **learns where to focus** by observing **human gaze, gestures, or cues**.\n",
    "- Helps the robot **identify relevant parts of the environment or task**.\n",
    "- Critical for **social learning** and improving **task performance efficiency**.\n",
    "- Can be combined with imitation or feedback to **guide learning priorities**.\n",
    "\n",
    "#### e) Learning from Human Preference\n",
    "- The robot **learns by comparing alternatives** based on human preferences.\n",
    "- Humans indicate **which option is better** among multiple candidate actions or outputs.\n",
    "- Enables **efficient learning from fewer examples** because feedback is **targeted and informative**.\n",
    "- Often combined with **preference learning algorithms** and **gradient-based updates**.\n",
    "\n",
    "#### f) Hierarchical Imitation\n",
    "- The robot **learns complex tasks by decomposing them into sub-tasks**.\n",
    "- Imitates human demonstrations at **different levels of abstraction** (high-level goals and low-level actions).\n",
    "- Useful for **structured or multi-step tasks**, allowing **modular learning and generalization**.\n",
    "- Can integrate with **task planning** and **BDI frameworks** for more efficient execution.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66dd8c",
   "metadata": {},
   "source": [
    "# Teaching and Learning Costs\n",
    "\n",
    "#### A) Imitation\n",
    "- Cost is mainly on the **human teacher**, who must provide demonstrations of the task.\n",
    "- The robot learns **passively**, so less human intervention is needed during learning.\n",
    "- **Challenges:** Requires high-quality demonstrations; errors in demonstration can propagate to the robot.\n",
    "\n",
    "#### B) Feedback\n",
    "- Cost is distributed between **human teacher and robot**.\n",
    "- Humans provide **evaluative or corrective signals** during learning, which may require ongoing attention.\n",
    "- The robot can **learn incrementally**, reducing the total number of required examples.\n",
    "- **Challenges:** Feedback must be **consistent and informative**; poorly timed or ambiguous feedback can slow learning.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5d6df",
   "metadata": {},
   "source": [
    "# Interactive Robot Learning Paradigm\n",
    "\n",
    "**Table 1.** Description of main Human Teaching Strategies. Robot action is performed at time-step $ t $. A teaching signal is the physical support of the strategy using social and/or task channels.\n",
    "\n",
    "| Categories        | Teaching signals  | Feedback             | Demonstration                          | Instruction             |\n",
    "|-------------------|-------------------|----------------------|----------------------------------------|-------------------------|\n",
    "| **Nature**        |   Notation        | $ H(s,a) $           | $D=\\{(s_t,a_t^*), (s_{t+1},a_{t+1}^*)...\\}$ | $ I_\\pi(s) = a_t^* $ |\n",
    "| **Nature**        |   Value           | Binary / Scalar      | State-Action pairs                     | Probability of an action |\n",
    "| **Time-step**     |   t-1             |                      | ✓                                      | ✓                       |\n",
    "| **Time-step**     |   t               |                      | ✓                                      |                         |\n",
    "| **Time-step**     |   t+1             | ✓                    |                                        |                         |\n",
    "| **Human**         |   Intention       | Evaluating or Correcting | Showing                            | Telling                 |\n",
    "| **Human**         |   Teaching cost   | Low                  | High                                   | Medium                  |\n",
    "| **Robot**         |   Interpretation  | State-Action evaluation (Reward-/Value-like) | Optimal actions (Policy-like) | Optimal action \n",
    "| **Robot**         |   Learning cost   | High                 | Low                                    | High                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b57bea",
   "metadata": {},
   "source": [
    "# Human Strategies\n",
    "\n",
    "Humans can adopt different strategies when teaching a robot or machine:\n",
    "\n",
    "- **Optimal teaching (boundary strategy):** Provide examples **closest to the decision boundary** to maximize learning efficiency.\n",
    "- **Extreme strategy:** Present examples **from easy to hard** or only the extremes of the spectrum.\n",
    "\n",
    "#### Example: Graspable Objects\n",
    "\n",
    "**Objective:** Classify objects as **graspable** or **not graspable**.\n",
    "\n",
    "**Three typical teaching behaviors:**\n",
    "1. **Extremes:** Only the most clear-cut examples; e.g., “this is very graspable” vs. “this is absolutely not graspable.” *Can be too binary...*\n",
    "2. **Linear:** Examples are presented **gradually from not graspable to graspable**, covering the spectrum. *High teaching cost...*\n",
    "3. **Positive only:** Only **graspable examples** are shown; the robot assumes everything else is non-graspable. *No negative can be a mistake...*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee7f2c",
   "metadata": {},
   "source": [
    "# Autonomous and Interactive Learning\n",
    "\n",
    "- **Goal:** Combine the strengths of\n",
    "  - **Autonomous learning:** the robot explores the environment on its own using **trial-and-error** to maximize rewards.\n",
    "  - **Interactive learning:** the robot learns from **human-provided guidance**, such as feedback, demonstrations, or preferences.\n",
    "\n",
    "#### Formal Framework: Markov Decision Process (MDP)\n",
    "\n",
    "**State, Action, Transition, Reward** (SATR) elements: \n",
    "- **State and Action Spaces:**\n",
    "  - $ s \\in S $ — state belongs to the state space $S$\n",
    "  - $ a \\in A $ — action belongs to the action space $A$\n",
    "- **State-Action Mapping (Transition Function):**\n",
    "  - $ T: S \\times A \\to S$ : Maps a **state-action pair** $(s, a)$ to the **next state** $s'$.\n",
    "- **Reward Function:**\n",
    "  - $ R: S \\times A \\to \\mathbb{R} $ : Assigns a **reward** for taking action $a$ in state $s$.\n",
    "- **Policy:**\n",
    "  - $ \\Pi(s)$ : Defines the **action to take** in each state.\n",
    "\n",
    "By **combining autonomous exploration with human guidance**, the robot can:\n",
    "- Explore the environment **on its own**.\n",
    "- Incorporate **targeted human input** to improve learning speed and accuracy.\n",
    "\n",
    "#### *Rest of ressources are availble here:*\n",
    "- *Mathematical formulas slide 26 [Open PDF: slides](InteractiveRobotLearning_slides.pdf)*\n",
    "- *Q-learning scheme slide 27 [Open PDF: slides](InteractiveRobotLearning_slides.pdf)*\n",
    "- *Shaping with evaluative feedback [Open PDF: slides](InteractiveRobotLearning_slides.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c300b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "628281ec",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
