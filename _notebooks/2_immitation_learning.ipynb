{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8e2eba",
   "metadata": {},
   "source": [
    "# MU5EEH15: Interactive Robot Learning\n",
    "\n",
    "Objective: Learn how to do programming in `Python` interactive robot learning.\n",
    "- Machine learning / Human Robot Interaction (HRI)\n",
    "- Reinforcement learning (rewards, human feedback)\n",
    "- Supervised learning\n",
    "- Immitation learning\n",
    "\n",
    "**Organization**: Lectures and Practical Labs **(TP 40%)** + Final project **(40%)** and exam **(20%)**\n",
    "\n",
    "**Lecturers**: \n",
    "- Silvia TULLI - mail: silvia.tulli@sorbonne-universite.fr\n",
    "- Hamed RAHIMI - mail: hamed.rahimi@sorbonne-universite.fr\n",
    "\n",
    "**Student**: William WU - mail: william.wu@etu.sorbonne-universite.fr\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a827c",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "\n",
    "By the end of this lecture, you should be able to:\n",
    "- Explain each of the key ingredients of\n",
    "imitation learning (demonstrations,\n",
    "environment, policy class, loss function,\n",
    "learning algorithm)\n",
    "- Define different imitation learning techniques, their strengths and limitations state visitation frequency, occupancy\n",
    "frequency and distributional shift\n",
    "- Explain the underlying differences between direct policy learning vs. Inverse Reinforcement Learning (IRL)\n",
    "- Explain different reward function representations in IRL\n",
    "- Define four main problems and solutions related to IRL:\n",
    "    - reward function ambiguity (degeneracy)\n",
    "    - expert suboptimality\n",
    "    - computational intensiveness\n",
    "- Master the following three learning algorithms:\n",
    "    - Behavioral Cloning (BC)\n",
    "    - DAgger\n",
    "    - Apprenticeship Learning IRL\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529afa6a",
   "metadata": {},
   "source": [
    "# 1) Behavioral Cloning Algorithm\n",
    "\n",
    "IL formulated as standard machine learning problem:\n",
    "- Fix a policy class\n",
    "    - e.g., support vector machine, neural network, decision tree, deep belief net...\n",
    "- Estimate a policy (= mapping from states to actions) from the training examples\n",
    "\n",
    "$D = {(s_0, a0), (s1, a_1), (s_2, a_2), ..., (s_n, a_n)}$\n",
    "\n",
    "### Distributional Shift\n",
    "\n",
    "Common assumption is that train and test set are independent and identically distributed (i.i.d.)\n",
    "However, $p_{\\pi*}(O_t) \\neq p_{\\pi_{\\theta}}(O_t)$\n",
    "\n",
    "#### Why does it matter ?\n",
    "\n",
    "- At each step, $BC$ policy has error rate $\\epsilon$ (e.g., 5% mistakes)\n",
    "- After $T$ steps, expected errors: $O(\\epsilon T ^ 2)$\n",
    "- Example: 1% error rate over 100\n",
    "steps ≈ 63% chance of failure\n",
    "\n",
    "#### Why i.i.d. breaks:\n",
    "\n",
    "- Supervised learning: test data ~ training data\n",
    "- Sequential decisions: $p(s_t|\\pi) \\neq p(s_t|\\pi*)$ after errors\n",
    "\n",
    "#### Behavioral Cloning (supervised learning):\n",
    "\n",
    "$ \\arg \\min \\mathbb{E}_{(s,a^*) \\sim P^*} L(a^*, \\pi_{\\theta}(s)) $ : Distribution provided exogenously\n",
    "\n",
    "#### (General) Imitation Learning:\n",
    "\n",
    "$ \\arg \\min \\mathbb{E}_{s \\sim P(s|\\theta)} L(\\pi^*(s), \\pi_{\\theta}(s)) $ : Distribution depends on the rollout $P(s|\\theta)$ = state distribution of $\\pi_{\\theta}$\n",
    "\n",
    "#### Types of Imitation Learning\n",
    "\n",
    "| Method | Direct Policy Learning | Reward Learning | Access to Environment | Interactive Demonstrator | Pre-collected Demonstrations |\n",
    "|--------|------------------------|-----------------|----------------------|-------------------------|-----------------------------|\n",
    "| Behavioral Cloning | Yes | No | No | No | Yes |\n",
    "| Direct Policy Learning (Interactive IL) | Yes | No | Yes | Yes | Optional |\n",
    "| Inverse Reinforcement Learning | No | Yes | Yes | No | Yes |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76048b5a",
   "metadata": {},
   "source": [
    "# 2) DAgger Algorithm\n",
    "\n",
    "Initialize $D \\leftarrow \\emptyset $.  \n",
    "Initialize $\\hat{\\pi}_1$ to any policy in $\\Pi$.  \n",
    "\n",
    "**for** $i = 1 $ **to** $N $ **do**  \n",
    " Let $\\pi_i = \\beta_i \\pi^* + (1 - \\beta_i) \\hat{\\pi}_i$.  \n",
    " Sample $T $-step trajectories using $\\pi_i$.  \n",
    " Get dataset $D_i = \\{(s, \\pi^*(s))\\} $ of visited states by $\\pi_i$ and actions given by expert.  \n",
    " Aggregate datasets: $D \\leftarrow D \\cup D_i $.  \n",
    " Train classifier $\\hat{\\pi}_{i+1}$ on $D $.  \n",
    "**end for**  \n",
    "\n",
    "Return best $\\hat{\\pi}_i$ on validation.\n",
    "\n",
    "#### Interactive Expert\n",
    "\n",
    "- Can query expert at any state\n",
    "- Construct loss function: $ L(\\pi^*(s), \\pi(s)) $\n",
    "- Typically applied to rollout trajectories of policies we are training: $ s \\sim P(s | \\pi) $\n",
    "- Driving example: $ L(\\pi^*(s), \\pi(s)) = (\\pi^*(s) - \\pi(s))^2 $\n",
    "\n",
    "#### BC vs Interactive IL: Concrete example\n",
    "\n",
    "| Method | Training Data | What Happens |\n",
    "|--------|---------------|--------------|\n",
    "| Behavioral Cloning | Only center-of-lane images | \"steer straight\" |\n",
    "| | Center images + near-edge images (from rollout) → expert corrections | Slight drift → edge of lane → NO DATA for recovery → crash |\n",
    "| | Center images + near-edge images (from rollout) → expert corrections | Slight drift → have data for \"steer back to center\" → recovers |\n",
    "\n",
    "#### When to Use Which Method\n",
    "\n",
    "| Criterion | Behavioral Cloning | DAgger | Apprenticeship IRL |\n",
    "|-----------|-------------------|--------|-------------------|\n",
    "| Expert availability | Offline dataset only | Must be interactive | Offline dataset |\n",
    "| Environment access | Not needed | Required | Required |\n",
    "| Best for | Large datasets, deterministic tasks | Critical applications | Transfer, understanding intent |\n",
    "| Computational cost | Low (one-time supervised learning) | Medium (iterative) | High (RL in inner loop) |\n",
    "| Sample efficiency | Poor (needs many demos) | Good (targeted queries) | Moderate |\n",
    "| Handles suboptimality | No | No | Yes (with modifications) |\n",
    "| Transfer to new tasks | Poor | Poor | Good |\n",
    "\n",
    "- BC: Large expert dataset, deterministic environments, offline learning\n",
    "- DAgger: Access to expert during training, expensive but effective\n",
    "- IRL: Transfer to new environments, expert suboptimality, understanding intent matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4fd94",
   "metadata": {},
   "source": [
    "# 3) Apprenticeship Learning via IRL\n",
    "\n",
    "<img src=\"picture_apprenticeship.png\">\n",
    "\n",
    "#### Problem setup\n",
    "\n",
    "**Input:**\n",
    "- State space $S$, action space $A$\n",
    "- Transition model: $P_{sa} (s_{t+1} | s_t, a_t)$\n",
    "- **MDP with no reward function**\n",
    "\n",
    "**Teacher's demonstration:**\n",
    "$ D = \\{(s_0, a_0), (s_1, a_1), (s_2, a_2), \\cdots, (s_n, a_n)\\} $\n",
    "\n",
    "Trace of the teacher's policy $\\pi^*$ that maps states to distributions over actions:\n",
    "$ \\pi^*(s) \\to P(s) $\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "### Behavioral Cloning or Direct Imitation:\n",
    "\n",
    "Can we directly learn the teacher's policy using supervised learning?\n",
    "\n",
    "- **Inverse RL:**  \n",
    "  Can we recover $R$ ?\n",
    "\n",
    "- **Apprenticeship learning via inverse RL:**  \n",
    "  Can we then use this $R$ to find a good policy?\n",
    "\n",
    "#### Problem setup\n",
    "\n",
    "**Rollout:** Sequentially execute $\\pi(s_0)$ on an initial state, produce trajectory:\n",
    "$ \\tau = s_0, a_0, s_1, a_1, \\ldots $\n",
    "\n",
    "#### $P(\\tau | \\pi)$: Distribution of trajectories induced by a policy\n",
    "\n",
    "1. Sample $s_0$ from $P_0$ (distribution over initial states)\n",
    "2. Initialize $t = 1$. Sample action $a_t$ from $\\pi(s_{t-1})$\n",
    "3. Sample next state $s_t$ from applying $a_t$ to $s_{t-1}$ (requires access to environment)\n",
    "4. Repeat from step 2 with $t = t + 1$\n",
    "\n",
    "#### $P(s | \\pi)$: Distribution of states induced by a policy\n",
    "\n",
    "1. Let $P_t(s|\\pi)$ denote distribution over $t$-th state\n",
    "2. $ P(s | \\pi) = \\frac{1}{T} \\sum_{t} P_t(s | \\pi) $\n",
    "\n",
    "#### Challenges\n",
    "- IRL is a undefined problem\n",
    "- It is difficult to evaluate a learned reward\n",
    "- Demonstrations might not be optimal\n",
    "\n",
    "___\n",
    "\n",
    "### Behavioral Cloning vs Inverse Control\n",
    "\n",
    "Behavioral Cloning has no notion of intention:\n",
    "- Expert suboptimality\n",
    "- Embodiment (correspondence problem)\n",
    "- Robustness\n",
    "\n",
    "Which has the most succinct description of a task\n",
    "$ \\pi^* \\text{ vs. } R? $\n",
    "\n",
    "Especially in planning oriented tasks, the reward function is often much more succinct than the optimal policy.\n",
    "\n",
    "Let $R(s) = w^T \\phi(s)$, where $w \\in \\mathbb{R}^n$ and $\\phi : S \\to \\mathbb{R}^n$\n",
    "\n",
    "$ E \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t) \\, | \\, \\pi \\right] = E \\left[ \\sum_{t=0}^{\\infty} \\gamma^t w^T \\phi(s_t) \\, | \\, \\pi \\right] $\n",
    "\n",
    "$ = w^T E \\left[ \\sum_{t=0}^{\\infty} \\gamma^t \\phi(s_t) \\, | \\, \\pi \\right] $\n",
    "\n",
    "$ = w^T \\mu(\\pi) $\n",
    "\n",
    "**Snipping into:** $ E \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R^*(s_t) \\, | \\, \\pi^* \\right] \\geq E \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R^*(s_t) \\, | \\, \\pi \\right] \\quad \\forall \\pi $\n",
    "\n",
    "Find $w^*$ such that $w^{*T} \\mu(\\pi^*) \\geq w^{*T} \\mu(\\pi) \\quad \\forall \\pi$\n",
    "\n",
    "Where $\\mu(\\pi)$ is the expected cumulative discounted sum of feature values or \"feature expectations\"\n",
    "\n",
    "___\n",
    "\n",
    "### Feature based reward function\n",
    "Grid Navigation Example:\n",
    "```cpp\n",
    "φ(s) =\n",
    "[ distance_to_goal(s),    // Feature vector (observable quantities)\n",
    "// Lower is better near_obstacle(s),\n",
    "// Avoid velocity(s),\n",
    "// Efficiency smoothness(s)\n",
    "// Comfort \n",
    "]\n",
    "\n",
    "// Learned weights (Weight vector (what matters?))\n",
    "w = [-1.0, -5.0, +0.5, +0.2]\n",
    "```\n",
    "\n",
    "#### Why linear?\n",
    "\n",
    "- Tractable: Convex optimization\n",
    "- Interpretable: See what expert values\n",
    "- Transferable: Same features, new weights for new tasks\n",
    "- Limited: Can't capture complex preferences\n",
    "\n",
    "Alternative representations:\n",
    "- Deep networks: $R(s) = NN(s)$\n",
    "- State-action: $R(s,a) = w^T \\phi(s,a)$\n",
    "- Trajectory: $R(\\tau) = f(s_0,...,s_T)$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3944e3",
   "metadata": {},
   "source": [
    "# Apprenticeship Learning via IRL\n",
    "\n",
    "$$ E \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R^*(s_t) \\mid \\pi^* \\right] \\geq E \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R^*(s_t) \\mid \\pi \\right] \\quad \\forall \\pi $$\n",
    "\n",
    "Let $R(s) = w^\\top \\phi(s)$, where $w \\in \\mathbb{R}^n$ and $\\phi : S \\to \\mathbb{R}^n$\n",
    "\n",
    "Find $w^*$ such that $w^{*\\top} \\mu(\\pi^*) \\geq w^{*\\top} \\mu(\\pi) \\quad \\forall \\pi$\n",
    "\n",
    "For a policy to be guaranteed to perform as well as the expert policy, it suffices that the feature expectations \"match\"\n",
    "\n",
    "#### Apprenticeship Learning via IRL Algorithm\n",
    "\n",
    "Let $R(s) = w^T \\phi(s)$, where $w \\in \\mathbb{R}^n$ and $\\phi : S \\to \\mathbb{R}^n$\n",
    "\n",
    "- Initialize some policy $\\Pi_0$\n",
    "- Iterate for $i = 1, 2, 3, \\ldots$\n",
    "  - Guess the reward: find a reward function such that the demonstrator policy maximally outperforms all previously found policies\n",
    "  - Find an optimal control policy $\\Pi_i$ for the current reward function\n",
    "    - If the expert is suboptimal, pick the best policy in a mixture\n",
    "  - Exit if $\\gamma \\geq \\epsilon/2$\n",
    "\n",
    "**Grid Example:** Expert always goes: S → → ↓ → G\n",
    "\n",
    "All these rewards make this optimal:\n",
    "\n",
    "1. $R_1(s) = 0$ everywhere (degenerate!)  \n",
    "2. $R_2(s) = +10$ at goal, 0 elsewhere  \n",
    "3. $R_3(s) = -\\text{distance}$ to goal  \n",
    "4. $R_4(s) = +100$ at goal, -1 per step  \n",
    "\n",
    "**Why this matters:** In supervised learning - one correct $f(x)=y$, in IRL - infinitely many correct $R(s)$\n",
    "\n",
    "**Degeneracy:** We have a lot of reward functions that could possibly be optimal, how do we pick one?\n",
    "\n",
    "#### Solutions to Degeneracy\n",
    "\n",
    "- Max Margin: Find R that makes expert better than alternatives by maximum margin\n",
    "- Maximum Entropy: Find R that makes expert have maximum entropy distribution\n",
    "- Bayesian: Posterior distribution over R given demos\n",
    "- Pragmatic: Just match feature expectations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629ef2f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
