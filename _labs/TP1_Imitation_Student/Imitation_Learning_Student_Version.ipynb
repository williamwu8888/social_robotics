{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrOtTxgMeTEM"
      },
      "source": [
        "- title: Imitation Learning\n",
        "- summary: step by step practical about BC\n",
        "- author: Silvia TULLI\n",
        "- feedback and revision: Kim BARAKA, Mohamed CHETOUANI\n",
        "- teaching assistant: Louis SIMON\n",
        "- date: 2025-09-23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrLYYqV_DTZy"
      },
      "source": [
        "This notebook contains an excerpt from the **Human-Interactive Robot Learning (HIRL)** educational module.\\\n",
        "For more information check out [our website](https://sites.google.com/view/hirl-education?usp=sharing).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGSV7aUaAjHs"
      },
      "source": [
        "The practical can be done alone or with a colleague. Please add below your information.\n",
        "\n",
        "Student(s):\\\n",
        "(1) NAME___________________ SURNAME___________________ ID___________________ Course___________________\\\n",
        "(2) NAME___________________ SURNAME___________________ ID___________________ Course___________________\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLrxTZc1ST32"
      },
      "source": [
        "# **Goals**\n",
        "* Understand and apply Imitation Learning (IL) techniques to simple tasks.\n",
        "* Implement direct Behavioral Cloning (BC) and a more advanced (interactive) IL algorithm.\n",
        "* Experiment with parameters such as the number of demonstrations and corrupted demonstrations to analyze their impact on performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv2O7DmpNt2H"
      },
      "source": [
        "# **Prerequisites**\n",
        "\n",
        "* mathematics for machine learning: linear algebra, calculus, probability and statitics\n",
        "* python programming for data science\n",
        "* lecture on interactive robot learning, in particular read the chapter Interactive Robot Learning (https://hal.science/hal-04060804/file/ACAI2021_chetouani_author-version.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elicxP-GMyPS"
      },
      "source": [
        "# **Imitation learning**\n",
        "\n",
        "Imitation Learning (IL) is a set of techniques aimed at training a model to directly mimic an expert's actions from a collection of demonstrations. In this practical, we focus on **offline imitation learning**, meaning we assume there exists a dataset consisting of demonstration data in which each sample correspond to a state-action pair collected from the expert. In practice, this might mean collecting keystrokes of a human player on an Atari game, controls of a human-driven car, or medical decisions of a doctor along a course of treatment. In this case, the goal of an IL algorithm is to learn a policy (i.e., a full mapping from states to actions) that mimics the expert policy (for which you only have a set of observations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUuXhR-mTC__"
      },
      "source": [
        "# **Part 1: Behavioral Cloning (BC)**\n",
        "\n",
        "Behavioral Cloning is a simple but effective imitation learning technique that involves training a model by learning a direct mapping from states to actions, without any intermediate representation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAd5gKc-Adxj"
      },
      "source": [
        "We're tackling imitation learning within a specific environment characterized by a discrete Markov Decision Process (MDP) with a fixed time horizon of $T$. There is an expert policy, denoted as $\\pi^*$, which provides deterministic actions at each state.\n",
        "\n",
        "The input of a Behavioral Cloning algorithm is a restricted policy class $\\Pi=\\{\\pi: S \\mapsto \\Delta(A)\\}$\n",
        "\n",
        "$$\n",
        "\\begin{array}{r}\n",
        "\\pi_\\theta =\\arg \\min _{\\pi \\in \\Pi} \\sum_{i=1}^{M} \\ell\\left(\\pi, s^{\\star}, a^{\\star}\\right) \\\\\n",
        "\\text { loss function }\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "We consider a set of human expert trajectories that adhere to $\\pi$ and we aim to create an imitation policy, $\\pi_\\theta$, that replicates these expert trajectories effectively. The objective is to ensure that for each state, the action chosen by our policy $\\pi_Î¸$ is exactly the same as the action chosen by the expert policy $\\pi$.\n",
        "\n",
        "In simpler terms, we aim to develop a policy that perfectly imitates the expert's actions in the given environment, making deterministic choices at each state to mimic the expert's behavior precisely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW3ryRBbFYy3"
      },
      "source": [
        "# **Environment Set up**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxqXIoMOXcoi"
      },
      "source": [
        "First, we define the environment, in this case a 5x5 gridworld. We also define the transition probabilities for each action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZmB3Eamq7f0"
      },
      "source": [
        "## Environment 1 definition\n",
        "\n",
        "The environment looks like this:\\\n",
        "![grid](img/grid.png)\n",
        "![action space](img/grid_action.png)\n",
        "![actions](img/action.png)\\\n",
        " Each state is represented by a number.\n",
        " The goal is to reach the `State 25` (i.e., flag cell) from any random state. There are three obstacles (i.e., TNT cells). The four actions that the agent can execute are: Up - Down - Left - Right. The agent cannot cross walls; therefore, the action space looks as pictured above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1kYTHkoHDtz"
      },
      "source": [
        "The transition matrix defines an MDP (Markov Decision Process) representing a system's dynamics and the impact of a specific action, labeled as `Action Up`. The matrix's rows correspond to distinct states within the MDP, ranging from state 1 (cell 0) to state 25 (cell 24). Each column represents a potential subsequent state that the system might transition to when `Action Up` is taken.\n",
        "\n",
        "Example of a transition matrix for Action Up (0)\n",
        "```\n",
        "#               1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25                \n",
        "Pu = np.array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 1\n",
        "               [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 2\n",
        "               [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 3\n",
        "               [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 4\n",
        "               [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 5\n",
        "               ...\n",
        "])\n",
        "```\n",
        "\n",
        "\n",
        "In the given context:\n",
        "\n",
        "The rows symbolize the current state of the system within the MDP, comprising $25$ distinct states.\n",
        "The columns represent the feasible future states that the system can transition to when `Action Up` is executed.\n",
        "The values in the matrix signify the probabilities of transitioning from the current state (row) to a specific subsequent state (column) upon taking `Action Up`.\n",
        "\n",
        "For instance, examining the first row `State 1`, it demonstrates a probability of $1.0$ (or $100$%) of transitioning to `State 1` when `Action Up` is performed. This implies that if the system is presently in `State 1` and `Action Up` is executed, it will unquestionably move to `State 1`.\n",
        "\n",
        "Conversely, examining the first column provides insights into the probabilities of transitioning to different states from any initial state when `Action Up` is employed. For `Action Up`, the system is more inclined to remain in the same state (diagonal elements are 1.0) and less likely to transition to other states (off-diagonal elements are 0.0).\n",
        "\n",
        "It's important to note that this MDP describes the system's probabilistic behavior in response to `Action Up` and is distinct from a standard Markov chain, as it incorporates the notion of making decisions (in this case, taking an action) and observing the subsequent state transitions.\n",
        "\n",
        "Below an example representation of an MDP from wikipedia:\\\n",
        "![wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/400px-Markov_Decision_Process.svg.png).\\\n",
        "\n",
        "Here you can observe that from $S0$ by performing $a0$ there is 50% of probability of staying in $S0$ and 50% of probability of going to $S2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULaK4ttGfbTo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Question 1.1**\n",
        "* Define a different environment and represent the environment dynamics as a MDP using a graph (pen and paper).\n",
        "\n",
        "* Specify the MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Question 1.2**\n",
        "You will now modify the `grid.py` file by\n",
        "\n",
        "* Writing a `print()` method to visualize the grid environment\n",
        "* Implementing a `set_mdp()` method to set the MDP as well as `reset()` method to initialize the state\n",
        "* Completing the `step()` using the MDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNlawwSKiriT"
      },
      "outputs": [],
      "source": [
        "from grid import GridWorldEnvironment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "fPxdYE8VHdve",
        "outputId": "a307d337-3d6a-478b-a007-66a290a788d2"
      },
      "outputs": [],
      "source": [
        "grid = (5,5)\n",
        "obstacle = np.random.randint(0,np.prod(grid),size=3)\n",
        "\n",
        "\n",
        "env = GridWorldEnvironment(grid_size=(5,5), obstacles=np.random.randint(0,24,size=3))\n",
        "env.reset()\n",
        "\n",
        "## Test print() and step()\n",
        "action = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
        "env.print()\n",
        "for i, a in enumerate(action):\n",
        "    print(\"Going\", a)\n",
        "    env.step(i)\n",
        "    env.print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5OVIkCUN_3H"
      },
      "source": [
        "# **Dataset Generation**\n",
        "Then we generate our dataset of expert demonstrations, which are represented as a state-action pairs $(s,a)$. In this example the expert agent attempts to move right when it's possible (i.e., not in the last column), and when it reaches the last column, it moves down. This policy is designed to guide the agent toward the goal state located in the bottom-right corner of the grid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Question 2.1**\n",
        "\n",
        "* Implement the *right-down* expert policy and generate demonstration\n",
        "* Print the expert demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmNy6wey0P6b",
        "outputId": "d4cf5a5c-51ce-4111-d683-236007f38def"
      },
      "outputs": [],
      "source": [
        "# Generate expert demonstrations\n",
        "def generate_expert_demonstrations(env, num_demos):\n",
        "    expert_demonstrations = []\n",
        "\n",
        "    for _ in range(num_demos):\n",
        "        state = env.reset()\n",
        "        trajectory = []\n",
        "\n",
        "        while state != env.state_space - 1:  # Continue until reaching the goal state\n",
        "            # Define a policy to reach the goal state while avoiding obstacles\n",
        "            action = simple_policy(env, state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            trajectory.append((state, action))\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "        if len(trajectory) > 0:\n",
        "            expert_demonstrations.append(trajectory)\n",
        "\n",
        "    return expert_demonstrations\n",
        "\n",
        "def simple_policy(env, state):\n",
        "    # A simple policy that avoids obstacles and follows a right-down strategy.\n",
        "    \n",
        "    \"\"\" Add your code here \"\"\"\n",
        "\n",
        "\n",
        "    \"\"\" \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    return action\n",
        "\n",
        "# Create the grid world environment\n",
        "env = GridWorldEnvironment()\n",
        "\n",
        "# Generate expert demonstrations\n",
        "num_demos = 10  # You can adjust the number of demonstrations\n",
        "expert_demonstrations = generate_expert_demonstrations(env, num_demos)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print the expert demonstrations\n",
        "\"\"\" Add your code here \"\"\"\n",
        "\n",
        "\n",
        "\"\"\" \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01q8FiazX-jM"
      },
      "source": [
        "Below we can observe that the demonstrations dataset comprises of state-action pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC8g9FwdX6t8",
        "outputId": "488edbc5-bec8-45f8-fbe4-37416ffac8aa"
      },
      "outputs": [],
      "source": [
        "expert_demonstrations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Question 2.2**\n",
        "Write a function `demo_summary()` with statistics about demonstration length, number of state visited, etc ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demo_summary(demo_list):\n",
        "\n",
        "    \"\"\" Add your code here \"\"\"\n",
        "\n",
        "\n",
        "    \"\"\" \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfhb8EVrPO82"
      },
      "source": [
        "### **Question 2.3**\n",
        "\n",
        "We provided demonstrations as they were given from a human.\n",
        "* Can you provide provide alternative demonstrations for this environment (e.g., based on an alternative policy)?\n",
        "* Can you suggest ways to generate these demonstrations?\n",
        "\n",
        "$\\rightarrow$ Play with the environment variables (e.g., number of obstacles) and the number of expert demonstrations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf4YrKzVTg6Z"
      },
      "source": [
        "# **Define the Behavioral Cloning Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfj1ficYT16C"
      },
      "source": [
        "## Model Structure\n",
        "The BehavioralCloningModel described below consists of a Neural Network with three fully connected (dense) layers:\\\n",
        "Input (1D) -> [fc1] -> [ReLU] -> [fc2] -> [ReLU] -> [fc3] -> Output (Mapping from states to deterministic actions)\n",
        "* `Input (1D)` represents the input layer with a single node, as state_dim is 1.\n",
        "* `[fc1]` represents the first fully connected layer with 64 neurons.\n",
        "* `[ReLU]` represents the Rectified Linear Unit (ReLU) activation function applied after each fully connected layer.\n",
        "* `[fc2]` represents the second fully connected layer with 64 neurons.\n",
        "* `[fc3]` represents the third fully connected layer, which outputs the mapping from states to deterministic actions. The number of neurons in this layer is determined by action_dim.\n",
        "\n",
        "The arrows between layers represent the connections and transformations of data as it flows through the network during forward pass. The ReLU activation functions introduce non-linearity in the model.\n",
        "\n",
        "The model learns a policy using **supervised learning** by minimizing the loss function. In behavioral cloning, the model is trained to predict actions that are as close as possible to the actions taken by the expert. Accuracy is then used to evaluate the model.\n",
        "\n",
        "In the code below, the **cross-entropy loss** quantifies the dissimilarity between the model's predicted action and the true actions from the expert.\n",
        "\n",
        "In the context of behavioral cloning for deterministic environments (where expert actions are treated as deterministic), the cross-entropy loss is used to train the model to predict actions that resemble the expert's actions. However, it's important to note that this approach assumes that the expert's actions are ground truth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJpxUSwPFdPL"
      },
      "source": [
        "Find out more about cross-entropy loss in [3.2.1.3 Log Loss Function](https://takaosa.github.io/paper/algorithmic-perspective-imitation.pdf).\n",
        "A general overview of imitation learning approaches, including different loss functions can be found in [Imitation Learning Lecture](https://web.stanford.edu/class/cs237b/pdfs/lecture/lecture_10111213.pdf) from Standford."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3oF0d052Khl"
      },
      "source": [
        "![architecture](img/archi.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Question 3.1**\n",
        "* What other loss function can be used? Why? Try them and explain your results\n",
        "* Add code to compute the accuracy at each epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HyDS0cTzYWq-",
        "outputId": "fb3a5cdb-6c1c-440d-a1f3-dd5a764e5106"
      },
      "outputs": [],
      "source": [
        "# Define the Behavioral Cloning Model\n",
        "class BehavioralCloningModel(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(BehavioralCloningModel, self).__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def train(self, model, states, actions, epochs=50, batch_size=32):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32)\n",
        "        # print('states', states)\n",
        "        print(\"States size\", states.size())\n",
        "\n",
        "        actions = torch.tensor(actions, dtype=torch.int64)\n",
        "        print(\"Actions size\", actions.size())\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(states, actions)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        accuracy_values = []\n",
        "        loss_values = []  # Initialize loss_values to track loss values\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            correct_predictions = 0\n",
        "\n",
        "            for batch_states, batch_actions in dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                predicted_actions = model(batch_states)\n",
        "                loss = loss_fn(predicted_actions, batch_actions)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Calculate the number of correct predictions in the current batch\n",
        "                correct_predictions += (predicted_actions.argmax(dim=1) == batch_actions).sum().item()\n",
        "\n",
        "            # Calculate accuracy for the current epoch and store it\n",
        "            \"\"\" Add your code here\"\"\"\n",
        "\n",
        "            \"\"\" \"\"\"\n",
        "\n",
        "            # Append the average loss for the current epoch to loss_values\n",
        "            loss_values.append(total_loss / len(dataloader))\n",
        "\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss_values[-1]:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "        # Calculate standard deviation for loss and accuracy\n",
        "        loss_std = np.std(loss_values)\n",
        "        accuracy_std = np.std(accuracy_values)\n",
        "\n",
        "        return loss_values, accuracy_values, loss_std, accuracy_std # Return loss and accuracy values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract states and actions from expert demonstrations\n",
        "states = []\n",
        "actions = []\n",
        "\n",
        "for demonstration in expert_demonstrations:\n",
        "    for state, action in demonstration:\n",
        "        states.append([state])  # Wrap the state in a list to make it 2D (1x1)\n",
        "        actions.append(action)\n",
        "\n",
        "# Define state_dim and action_dim based on your data\n",
        "state_dim = 1  # Assuming state is a scalar value\n",
        "action_dim = max(actions) + 1  # Calculate action_dim based on the maximum action value\n",
        "\n",
        "# Instantiate the model\n",
        "model = BehavioralCloningModel(state_dim, action_dim)\n",
        "\n",
        "# Train the model with the expert demonstration data\n",
        "loss_values, accuracy_values, loss_std, accuracy_std = model.train(model, states, actions, epochs=500, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "vcIRUIqmsCCL",
        "outputId": "96e3e9c1-a52b-4e31-929b-025c022048c7"
      },
      "outputs": [],
      "source": [
        "epochs = len(accuracy_values)\n",
        "# Define lighter colors for the standard deviation lines\n",
        "lighter_blue = \"b\"\n",
        "lighter_red = \"r\"\n",
        "\n",
        "# Plot Loss and Accuracy\n",
        "\"\"\" Add you code here \"\"\"\n",
        "\n",
        "\n",
        "\"\"\" \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "djqnEAzLtZj3",
        "outputId": "ad1a4a3b-d0e6-48e7-d7b8-7218978188d8"
      },
      "outputs": [],
      "source": [
        "# Visualize the Distribution of Actions\n",
        "action_counts = [actions.count(i) for i in range(action_dim)]\n",
        "plt.bar(range(action_dim), action_counts)\n",
        "plt.xticks(range(action_dim))\n",
        "plt.xlabel(\"Action\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Actions\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWCGE6_E8pRd"
      },
      "source": [
        "## Rollout policy\n",
        "\n",
        "We evaluate the efficiency of the learned model by using it as a policy in the RL environment and logging the final reward for multiple episodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Question 3.2**\n",
        "Implement policy rollout\n",
        "* Select action from the neural network \n",
        "* Execute action the environment\n",
        "* Update `done`, `reward`, and `step` variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB4swlcz8pAl"
      },
      "outputs": [],
      "source": [
        "def policy_rollout(env, bc_policy, n_episode=10, max_step=100):\n",
        "\n",
        "  avg_reward, avg_nstep = [], []\n",
        "\n",
        "  for e in range(n_episode):\n",
        "\n",
        "    #Starting one episode\n",
        "    env.reset()\n",
        "    reward, step = 0, 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "      \"\"\" Add your code here \"\"\"\n",
        "\n",
        "      # Select action with behavioral cloning policy\n",
        "\n",
        "\n",
        "      # Execute action\n",
        "   \n",
        "\n",
        "\n",
        "      # Update done, reward, and step\n",
        "\n",
        "\n",
        "\n",
        "      \"\"\" \"\"\"\n",
        "\n",
        "      if step == max_step:\n",
        "        reward = -5\n",
        "        break\n",
        "\n",
        "\n",
        "    avg_reward.append(reward)\n",
        "    avg_nstep.append(step)\n",
        "\n",
        "  return avg_reward, avg_nstep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd8pap1uJGck",
        "outputId": "b5312223-1f38-450d-dd4d-fbfb9654b7bd"
      },
      "outputs": [],
      "source": [
        "avg_reward, avg_nstep = policy_rollout(env, model, 50)\n",
        "print(f\"Steps {np.mean(avg_nstep)} / {np.std(avg_nstep):.3f}\")\n",
        "print(f\"Reward {np.mean(avg_reward)} / {np.std(avg_reward):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Bonus**: *Stochastic-Icy-MDP*\n",
        "![charlie_brown](img/charlie_brown.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that while the term *probabilities* might imply a probabilistic policy, in this deterministic case, these values represent the model's confidence or preference for each possible action given a particular state.\n",
        "\n",
        "Now, consider an *icy*-environment with some inertia. For example, if the agent positioned at `state = 0` goes to the right, it might end up in state 1, 2, 3, or even 4! The probability of reaching a state on the same column or row decreases with respect to the distance to the initial state. For example :\n",
        "\n",
        "$$ \\mathcal{P}([1,2,3,4] | s=0, a=right) = [0.8, 0.1, 0.08, 0.02] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Question 4**\n",
        "* Define a new `set_mdp()` method in the `IcyGridWorldEnvironment` class and run experiment again with this stochastic environment. Make sure that the probability sums to 1, *i.e.*, $\\sum_{s_{t+1}}\\mathcal{P}(s_{t+1}|s=s, a=a) = 1$.\n",
        "\n",
        "**Hint**: To define transition probabilities that sum to 1, you can either use a softmax or a power law."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" Initialize a IcyGridWorldEnvironment and run Behavioral Cloning\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJWIYQGOpy98"
      },
      "source": [
        "# **Open-ended Project**\n",
        "Compare the performance of Behavioral Cloning with another Imitation Learning algorithm of your choice (e.g., IRL, DAgger).\n",
        "To choose your alternative algorithm, feel free to check out the implementation offered by [imitation](https://imitation.readthedocs.io/en/latest/algorithms/bc.html).\n",
        "* Do a benchmark of similar existing projects/approaches.\n",
        "* Together with the alternative algorithm of your choice, provide details about how the algorithm works.\n",
        "* Discuss your results and highlighting the advantages and limitations of your approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHkFWcvrDI9o"
      },
      "source": [
        "# **References**\n",
        "* D. A. Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3:88â97, 1991.\n",
        "* F. Torabi, G. Warnell, and P. Stone. Behavioral cloning from observation. ArXiv, abs/1805.01954, 2018.\n",
        "* [Imitation Learning open source library](https://imitation.readthedocs.io/en/latest/algorithms/bc.html)\n",
        "* CS 285 at UC Berkeley, [Deep Reinforcement Learning](https://rail.eecs.berkeley.edu/deeprlcourse/)\n",
        "* M. Chetouani. Interactive Robot Learning: An Overview. Chetouani, M.; Dignum, V.; Lukow- icz, P.; Sierra, C. Human-Centered Artificial Intelligence, 13500, Springer International Publishing, pp.140-172, 2023, Lecture Notes in Computer Science, 10.1007/978-3-031-24349-3_9 . [hal-04060804](https://hal.science/hal-04060804/file/ACAI2021_chetouani_author-version.pdf)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
