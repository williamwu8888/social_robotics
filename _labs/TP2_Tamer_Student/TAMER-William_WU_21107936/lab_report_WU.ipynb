{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f3fcd28",
   "metadata": {},
   "source": [
    "# MU5EEH15: Interactive Robot Learning\n",
    "\n",
    "## Social Robotics - TP2: Learning From Human Feedback\n",
    "\n",
    "**Objective**: Comprehensive Analysis of TAMER Framework and Enhancements\n",
    "\n",
    "**Teachers and instructors**: \n",
    "- Mohamed CHETOUANI - mail: mohamed.chetouani@sorbonne-universite.fr\n",
    "- Louis SIMON - mail: louis.simon@isir.upmc.fr\n",
    "- Silvia TULLI - mail:tulli@isir.upmc.fr\n",
    "\n",
    "**Student**: William WU - mail: william.wu@etu.sorbonne-universite.fr\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7b361",
   "metadata": {},
   "source": [
    "This report presents a comprehensive analysis and enhancement of the TAMER (Training an Agent Manually via Evaluative Reinforcement) framework, originally proposed by Knox & Stone (2009). The original implementation provided a foundation for human-in-the-loop reinforcement learning, where human evaluative feedback replaces traditional environmental reward signals. Through systematic exploration and enhancement of this framework, this work addresses all four questions posed in the assignment while maintaining backward compatibility with the provided codebase.\n",
    "\n",
    "The enhancements focus on creating a modular, extensible architecture that facilitates experimentation with different environments, learning algorithms, and interaction modalities. By building upon the original MountainCar-v0 implementation, this work demonstrates practical applications of TAMER principles while providing clear theoretical insights into the fundamental differences between human-guided and environment-driven learning approaches. The resulting framework serves as both a practical tool for experimentation and a demonstration of key concepts in interactive machine learning for social robotics applications.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3109e0b",
   "metadata": {},
   "source": [
    "## Code Structure\n",
    "\n",
    "```bash\n",
    "TAMER-enhanced/\n",
    "├── run_enhanced.py               # Main experiment runner\n",
    "├── run.py                        # Original (preserved)\n",
    "├── tamer/\n",
    "│   ├── agent_enhanced.py         # Enhanced TAMER with multiple variants\n",
    "│   ├── interface_enhanced.py     # Multi-modal feedback interfaces\n",
    "│   ├── environment_manager.py    # Environment handling & compatibility\n",
    "│   ├── config.py                 # Experiment configurations\n",
    "│   ├── agent.py                  # Original (preserved)\n",
    "│   └── interface.py              # Original (preserved)\n",
    "├── logs/                         # Enhanced logging & analysis\n",
    "├── saved_models/                 # Trained model storage\n",
    "└── requirements.txt              # Required libraries to be installed\n",
    "```\n",
    "\n",
    "### Key Technical Improvements\n",
    "\n",
    "- Modular Design: Separated concerns between learning, interface, and environment management\n",
    "\n",
    "- Comprehensive Logging: Detailed performance tracking and analysis capabilities\n",
    "\n",
    "- Configuration Management: Centralized experiment settings for reproducibility\n",
    "\n",
    "- Error Handling: Robust exception handling and recovery mechanisms\n",
    "\n",
    "- Performance Monitoring: Real-time training progress and model convergence tracking\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb68056",
   "metadata": {},
   "source": [
    "## Question 1: Environment Exploration\n",
    "\n",
    "### Theoretical Analysis\n",
    "\n",
    "The assignment required exploring different OpenAI Gym environments to understand their observation spaces, action spaces, rewards, and termination conditions. Through systematic analysis, several environments were evaluated for TAMER compatibility:\n",
    "\n",
    "- MountainCar-v0: 2D continuous observation (position, velocity).\n",
    "    > 3 discrete actions, -1 reward per timestep, terminates at position ≥ 0.5 or 200 steps\n",
    "\n",
    "- CartPole-v1: 4D continuous observation.\n",
    "    > 2 discrete actions, +1 reward per step while pole upright, terminates when pole angle > 12° or position > 2.4\n",
    "\n",
    "- MountainCarContinuous-v0: Similar to discrete version but with continuous action space (-1.0 to 1.0)\n",
    "\n",
    "- Pendulum-v1: 3D continuous observation, continuous action space, complex reward function\n",
    "\n",
    "The analysis revealed that environments with discrete action spaces are most suitable for TAMER, while continuous action spaces present significant challenges for binary human feedback.\n",
    "\n",
    "### Implementation & Code Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3255f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Exploration Implementation\n",
    "class EnvironmentManager:\n",
    "    \"\"\"Manages different Gym environments for TAMER experimentation\"\"\"\n",
    "    \n",
    "    def list_compatible_environments(self):\n",
    "        \"\"\"List environments suitable for TAMER\"\"\"\n",
    "        compatible = []\n",
    "        for env_name, info in self.supported_environments.items():\n",
    "            if info['suitable_for_tamer']:\n",
    "                compatible.append({\n",
    "                    'name': env_name,\n",
    "                    'description': info['description'],\n",
    "                    'action_space': info['action_space']\n",
    "                })\n",
    "        return compatible\n",
    "    \n",
    "    def create_environment(self, env_name, render_mode='rgb_array'):\n",
    "        \"\"\"Create and return a Gym environment with proper configuration\"\"\"\n",
    "        env_info = self.supported_environments[env_name]\n",
    "        \n",
    "        if not env_info['suitable_for_tamer']:\n",
    "            print(f\"Warning: {env_name} may not be ideal for TAMER\")\n",
    "        \n",
    "        env = gym.make(env_name, render_mode=render_mode)\n",
    "        print(f\"Created environment: {env_name}\")\n",
    "        print(f\"  Action Space: {env.action_space}\")\n",
    "        print(f\"  Observation Space: {env.observation_space}\")\n",
    "        return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3746c91",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "- MountainCar-v0 proved most suitable due to simple discrete action space and clear learning objective\n",
    "\n",
    "- CartPole-v1 showed potential but required more sophisticated feedback strategies\n",
    "\n",
    "- Continuous action spaces (MountainCarContinuous, Pendulum) were incompatible with basic TAMER implementation\n",
    "\n",
    "- Performance comparison showed TAMER's superiority over pure Q-learning in sample efficiency\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548ccc1",
   "metadata": {},
   "source": [
    "## Question 2: Algorithm Variants\n",
    "\n",
    "### Theoretical Framework\n",
    "\n",
    "TAMER's core innovation lies in replacing the environmental reward function R(s,a) with a human reward model Ĥ(s,a). Several variants were implemented to explore different human feedback incorporation strategies:\n",
    "\n",
    "1. Pure TAMER: Original formulation using only human feedback\n",
    "\n",
    "2. Hybrid Approach: Combines human feedback with environmental rewards\n",
    "\n",
    "3. Transfer Learning: Leverages both models with adaptive weighting\n",
    "\n",
    "4. Q-learning Baseline: Standard reinforcement learning for comparison\n",
    "\n",
    "### Implementation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ac73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TamerEnhanced:\n",
    "    \"\"\"Enhanced TAMER agent implementing multiple variants\"\"\"\n",
    "    \n",
    "    def _mode_specific_learning(self, state, action, env_reward, next_state, done, disp, timestep):\n",
    "        \"\"\"Different learning updates for each variant\"\"\"\n",
    "        \n",
    "        if self.mode == 'tamer':\n",
    "            # Pure human feedback learning\n",
    "            human_reward = self._collect_human_feedback(disp)\n",
    "            if human_reward != 0:\n",
    "                error = self.H.update(state, action, human_reward)\n",
    "                \n",
    "        elif self.mode == 'hybrid':\n",
    "            # Combined human and environment learning\n",
    "            human_reward = self._collect_human_feedback(disp)\n",
    "            td_target = self._compute_td_target(state, action, next_state, env_reward, done)\n",
    "            \n",
    "            # Update both models\n",
    "            q_error = self.Q.update(state, action, td_target)\n",
    "            if human_reward != 0:\n",
    "                h_error = self.H.update(state, action, human_reward)\n",
    "                \n",
    "        elif self.mode == 'transfer':\n",
    "            # Transfer learning between models\n",
    "            human_reward = self._collect_human_feedback(disp)\n",
    "            td_target = self._compute_td_target(state, action, next_state, env_reward, done)\n",
    "            \n",
    "            # Adaptive combination for action selection\n",
    "            q_preds = self.Q.predict(state)\n",
    "            h_preds = self.H.predict(state)\n",
    "            combined = [\n",
    "                (1 - self.transfer_alpha) * q + self.transfer_alpha * h \n",
    "                for q, h in zip(q_preds, h_preds)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea77e6",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "\n",
    "The hybrid approach demonstrated the most robust performance:\n",
    "\n",
    "- Pure TAMER: Fast initial learning but dependent on human consistency\n",
    "\n",
    "- Hybrid Mode: Balanced performance, benefiting from both guidance sources\n",
    "\n",
    "- Transfer Learning: Adaptive weighting showed promise for long-term learning\n",
    "\n",
    "- Q-learning: Slowest learning but most consistent without human input\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32866ee",
   "metadata": {},
   "source": [
    "## Question 3: Feedback Interfaces\n",
    "\n",
    "### Interface Design Considerations\n",
    "\n",
    "The original binary keyboard interface (W/A keys) was extended to address limitations in expressivity and usability:\n",
    "\n",
    "- Limited granularity: Binary feedback insufficient for nuanced guidance\n",
    "\n",
    "- Cognitive load: Continuous attention requirement\n",
    "\n",
    "- Feedback delay: Human reaction time vs. agent learning speed\n",
    "\n",
    "- Ergonomics: Physical interface constraints\n",
    "\n",
    "### Enhanced Interface Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce62454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedInterface:\n",
    "    \"\"\"Enhanced interface supporting multiple feedback methods\"\"\"\n",
    "    \n",
    "    def _setup_enhanced_keyboard(self):\n",
    "        \"\"\"Multi-level keyboard feedback\"\"\"\n",
    "        self.key_mapping = {\n",
    "            pygame.K_w: 2.0,    # Strong positive\n",
    "            pygame.K_e: 1.0,    # Medium positive\n",
    "            pygame.K_q: 0.5,    # Weak positive\n",
    "            pygame.K_a: -2.0,   # Strong negative\n",
    "            pygame.K_d: -1.0,   # Medium negative  \n",
    "            pygame.K_s: -0.5,   # Weak negative\n",
    "            pygame.K_SPACE: 0.0 # Explicit neutral\n",
    "        }\n",
    "    \n",
    "    def get_feedback(self):\n",
    "        \"\"\"Enhanced feedback collection with visualization\"\"\"\n",
    "        reward = 0\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.KEYDOWN and event.key in self.key_mapping:\n",
    "                reward = self.key_mapping[event.key]\n",
    "                self._visualize_feedback(reward)\n",
    "                break\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7a39e",
   "metadata": {},
   "source": [
    "### Alternative Interface Proposals\n",
    "\n",
    "1. Analog Joystick: Continuous feedback range, more natural for control tasks\n",
    "\n",
    "    - Advantages: Intuitive, proportional control, familiar gaming interface\n",
    "\n",
    "    - Disadvantages: Hardware requirement, setup complexity\n",
    "\n",
    "2. Touchscreen Gestures: Swipe-based interaction with direction and intensity\n",
    "\n",
    "    - Advantages: Highly intuitive, multi-touch capable, mobile compatibility\n",
    "\n",
    "    - Disadvantages: Visual attention required, development complexity\n",
    "\n",
    "3. Voice Commands: Natural language feedback with intensity levels\n",
    "\n",
    "    - Advantages: Hands-free, natural interaction, accessible\n",
    "\n",
    "    - Disadvantages: Speech recognition challenges, background noise sensitivity\n",
    "\n",
    "4. Biometric Sensors: Physiological response measurement (GSR, heart rate)\n",
    "\n",
    "    - Advantages: Passive feedback, real emotional response\n",
    "\n",
    "    - Disadvantages: Specialized hardware, signal interpretation complexity\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fac167",
   "metadata": {},
   "source": [
    "## Question 4: Reward vs Human Feedback\n",
    "\n",
    "### Three Fundamental Differences\n",
    "\n",
    "1. Origin and Nature of Signal\n",
    "\n",
    "- Reward Function R(s,a):\n",
    "\n",
    "    - Predefined by environment designer\n",
    "\n",
    "    - Objective, deterministic, and automatic\n",
    "\n",
    "    - Consistent across identical state-action pairs\n",
    "\n",
    "    - Scales effectively to complex environments\n",
    "\n",
    "- Human Feedback H(s,a):\n",
    "\n",
    "    - Subjective human evaluation\n",
    "\n",
    "    - Variable, stochastic, and manually provided\n",
    "\n",
    "    - Inconsistent between evaluators and sessions\n",
    "\n",
    "    - Impractical to collect for large state spaces\n",
    "\n",
    "2. Temporal Characteristics\n",
    "\n",
    "- Reward Signal:\n",
    "\n",
    "    - Immediate delivery after action\n",
    "\n",
    "    - Focused on current state-action pair\n",
    "\n",
    "    - Minimal computational delay\n",
    "\n",
    "- Human Feedback:\n",
    "\n",
    "    - Variable delay (200-1000ms human reaction time)\n",
    "\n",
    "    - May consider action consequences beyond immediate state\n",
    "\n",
    "    - Subject to human attention and cognitive processing\n",
    "\n",
    "3. Learning Update Mechanisms\n",
    "\n",
    "- Q-learning Update Rule:\n",
    "    ```python\n",
    "    # Temporal Difference Learning with bootstrapping\n",
    "    td_target = R(s,a) + γ * max(Q(s',a'))\n",
    "    Q(s,a) = Q(s,a) + α * [td_target - Q(s,a)]\n",
    "    ```\n",
    "\n",
    "\n",
    "- TAMER Update Rule:\n",
    "    ```python\n",
    "    # Direct supervised learning update\n",
    "    H(s,a) = H(s,a) + α * [human_feedback - H(s,a)]\n",
    "    ```\n",
    "\n",
    "### Additional Theoretical Distinctions\n",
    "\n",
    "4. Generalization Mechanisms\n",
    "\n",
    "- Reward-based Generalization:\n",
    "\n",
    "    - Achieved through value function learning\n",
    "\n",
    "    - Bootstrapping propagates information gradually\n",
    "\n",
    "    - Requires multiple experiences for reliable generalization\n",
    "\n",
    "- Feedback-based Generalization:\n",
    "\n",
    "    - Immediate through RBF feature similarity\n",
    "\n",
    "    - Single feedback instance influences similar states\n",
    "\n",
    "    - Faster but potentially less accurate generalization\n",
    "\n",
    "5. Scalability and Practical Considerations\n",
    "\n",
    "- Reward Functions:\n",
    "\n",
    "    - Automatic scaling to complex environments\n",
    "\n",
    "    - Consistent performance across episodes\n",
    "\n",
    "    - Limited by environment design quality\n",
    "\n",
    "- Human Feedback:\n",
    "\n",
    "    - Human bottleneck for complex tasks\n",
    "\n",
    "    - Quality varies with fatigue and attention\n",
    "\n",
    "    - Costly and time-consuming for large-scale training\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352ed42",
   "metadata": {},
   "source": [
    "## Conclusion & Future Directions\n",
    "\n",
    "This investigation demonstrates TAMER's practical viability for human-in-the-loop reinforcement learning while revealing crucial design considerations. The environmental analysis establishes discrete action spaces as essential for effective human feedback integration, while algorithmic comparisons show hybrid approaches outperform pure methods by leveraging both human guidance and environmental rewards.\n",
    "\n",
    "The interface enhancements prove that interaction design significantly impacts learning efficiency, with multi-level feedback systems enabling more nuanced teaching. The theoretical analysis clarifies fundamental distinctions between reward-based and feedback-based learning, providing guidance for appropriate application contexts.\n",
    "\n",
    "Promising research trajectories include active learning strategies that optimize human cognitive load, multi-modal feedback integration for richer communication, and longitudinal adaptation to individual teaching styles. The application of explainable AI techniques could enhance transparency in human-guided learning systems.\n",
    "\n",
    "These advancements point toward more adaptive robotic systems capable of learning complex behaviors through natural human interaction, bridging the gap between machine efficiency and human intuition in social robotics applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fac4c9",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# References\n",
    "\n",
    "1. Knox, W.B., & Stone, P. (2009). Interactively shaping agents via human reinforcement: the TAMER framework\n",
    "\n",
    "2. Knox, W.B., & Stone, P. (2012). Reinforcement learning from simultaneous human and MDP reward\n",
    "\n",
    "3. Wirth, C., et al. (2017). A survey of preference-based reinforcement learning methods\n",
    "\n",
    "4. Christiano, P.F., et al. (2017). Deep reinforcement learning from human preferences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
